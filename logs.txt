
==> Audit <==
|------------|-----------------------|----------|--------|---------|---------------------|---------------------|
|  Command   |         Args          | Profile  |  User  | Version |     Start Time      |      End Time       |
|------------|-----------------------|----------|--------|---------|---------------------|---------------------|
| start      |                       | minikube | stepan | v1.34.0 | 04 Dec 24 22:22 MSK | 04 Dec 24 22:24 MSK |
| image      | load flask-app:latest | minikube | stepan | v1.34.0 | 04 Dec 24 22:52 MSK | 04 Dec 24 22:53 MSK |
| service    | flask-service --url   | minikube | stepan | v1.34.0 | 04 Dec 24 22:55 MSK |                     |
| image      | load flask-app:latest | minikube | stepan | v1.34.0 | 04 Dec 24 22:56 MSK | 04 Dec 24 22:57 MSK |
| image      | ls                    | minikube | stepan | v1.34.0 | 04 Dec 24 22:58 MSK | 04 Dec 24 22:58 MSK |
| docker-env |                       | minikube | stepan | v1.34.0 | 04 Dec 24 22:59 MSK | 04 Dec 24 22:59 MSK |
| docker-env |                       | minikube | stepan | v1.34.0 | 04 Dec 24 23:00 MSK | 04 Dec 24 23:00 MSK |
| image      | load flask-app:latest | minikube | stepan | v1.34.0 | 04 Dec 24 23:00 MSK | 04 Dec 24 23:01 MSK |
| service    | flask-service --url   | minikube | stepan | v1.34.0 | 04 Dec 24 23:02 MSK |                     |
| docker-env |                       | minikube | stepan | v1.34.0 | 04 Dec 24 23:04 MSK | 04 Dec 24 23:04 MSK |
| image      | ls                    | minikube | stepan | v1.34.0 | 04 Dec 24 23:05 MSK | 04 Dec 24 23:05 MSK |
| image      | load flask-app:new    | minikube | stepan | v1.34.0 | 04 Dec 24 23:05 MSK | 04 Dec 24 23:05 MSK |
| docker-env |                       | minikube | stepan | v1.34.0 | 04 Dec 24 23:09 MSK | 04 Dec 24 23:09 MSK |
| image      | load flask-app:latest | minikube | stepan | v1.34.0 | 04 Dec 24 23:09 MSK | 04 Dec 24 23:10 MSK |
| stop       |                       | minikube | stepan | v1.34.0 | 04 Dec 24 23:12 MSK | 04 Dec 24 23:12 MSK |
| start      |                       | minikube | stepan | v1.34.0 | 04 Dec 24 23:12 MSK | 04 Dec 24 23:13 MSK |
| service    | flask-service --url   | minikube | stepan | v1.34.0 | 04 Dec 24 23:14 MSK |                     |
|------------|-----------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/12/04 23:12:49
Running on machine: Stepans-MacBook-Pro
Binary: Built with gc go1.23.1 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1204 23:12:49.447025   17208 out.go:345] Setting OutFile to fd 1 ...
I1204 23:12:49.447173   17208 out.go:397] isatty.IsTerminal(1) = true
I1204 23:12:49.447175   17208 out.go:358] Setting ErrFile to fd 2...
I1204 23:12:49.447178   17208 out.go:397] isatty.IsTerminal(2) = true
I1204 23:12:49.447393   17208 root.go:338] Updating PATH: /Users/stepan/.minikube/bin
I1204 23:12:49.447427   17208 oci.go:576] shell is pointing to dockerd inside minikube. will unset to use host
W1204 23:12:49.447513   17208 root.go:314] Error reading config file at /Users/stepan/.minikube/config/config.json: open /Users/stepan/.minikube/config/config.json: no such file or directory
I1204 23:12:49.447745   17208 out.go:352] Setting JSON to false
I1204 23:12:49.473273   17208 start.go:129] hostinfo: {"hostname":"Stepans-MacBook-Pro.local","uptime":888322,"bootTime":1732454847,"procs":334,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.3","kernelVersion":"23.3.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"f56a2390-ecc8-5b80-8857-915ef93e1db3"}
W1204 23:12:49.473570   17208 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1204 23:12:49.479662   17208 out.go:177] 😄  minikube v1.34.0 on Darwin 14.3 (arm64)
I1204 23:12:49.490210   17208 out.go:177]     ▪ MINIKUBE_ACTIVE_DOCKERD=minikube
I1204 23:12:49.490582   17208 notify.go:220] Checking for updates...
I1204 23:12:49.497876   17208 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1204 23:12:49.498450   17208 driver.go:394] Setting default libvirt URI to qemu:///system
I1204 23:12:49.563592   17208 docker.go:123] docker version: linux-25.0.3:Docker Desktop 4.28.0 (139021)
I1204 23:12:49.563948   17208 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1204 23:12:49.831769   17208 info.go:266] docker info: {ID:4b86e550-a026-435f-9e37-18475c25a5af Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:77 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:52 OomKillDisable:false NGoroutines:89 SystemTime:2024-12-04 20:12:49.808613797 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8222203904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/stepan/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:/Users/stepan/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:/Users/stepan/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:/Users/stepan/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/stepan/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:/Users/stepan/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/stepan/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:/Users/stepan/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/stepan/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I1204 23:12:49.836495   17208 out.go:177] ✨  Using the docker driver based on existing profile
I1204 23:12:49.844491   17208 start.go:297] selected driver: docker
I1204 23:12:49.844493   17208 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1204 23:12:49.844528   17208 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1204 23:12:49.844863   17208 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1204 23:12:49.944587   17208 info.go:266] docker info: {ID:4b86e550-a026-435f-9e37-18475c25a5af Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:77 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:52 OomKillDisable:false NGoroutines:89 SystemTime:2024-12-04 20:12:49.929056839 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8222203904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/stepan/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:/Users/stepan/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:/Users/stepan/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:/Users/stepan/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/stepan/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:/Users/stepan/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/stepan/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:/Users/stepan/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/stepan/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I1204 23:12:49.944843   17208 cni.go:84] Creating CNI manager for ""
I1204 23:12:49.945146   17208 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1204 23:12:49.945186   17208 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1204 23:12:49.948620   17208 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1204 23:12:49.952676   17208 cache.go:121] Beginning downloading kic base image for docker with docker
I1204 23:12:49.956528   17208 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1204 23:12:49.963714   17208 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1204 23:12:49.963715   17208 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1204 23:12:49.963773   17208 preload.go:146] Found local preload: /Users/stepan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4
I1204 23:12:49.963785   17208 cache.go:56] Caching tarball of preloaded images
I1204 23:12:49.963915   17208 preload.go:172] Found /Users/stepan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I1204 23:12:49.963921   17208 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1204 23:12:49.964001   17208 profile.go:143] Saving config to /Users/stepan/.minikube/profiles/minikube/config.json ...
W1204 23:12:50.050938   17208 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1204 23:12:50.050952   17208 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1204 23:12:50.051044   17208 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1204 23:12:50.051065   17208 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1204 23:12:50.051321   17208 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1204 23:12:50.051335   17208 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1204 23:12:50.051340   17208 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1204 23:12:50.354523   17208 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1204 23:12:50.354600   17208 cache.go:194] Successfully downloaded all kic artifacts
I1204 23:12:50.354638   17208 start.go:360] acquireMachinesLock for minikube: {Name:mkb80327a3cc7e7515e66c19950c2318ced6f0b4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1204 23:12:50.354786   17208 start.go:364] duration metric: took 118.583µs to acquireMachinesLock for "minikube"
I1204 23:12:50.354822   17208 start.go:96] Skipping create...Using existing machine configuration
I1204 23:12:50.354827   17208 fix.go:54] fixHost starting: 
I1204 23:12:50.359748   17208 out.go:177] 📌  Noticed you have an activated docker-env on docker driver in this terminal:
W1204 23:12:50.363577   17208 out.go:270] ❗  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I1204 23:12:50.363799   17208 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 23:12:50.416960   17208 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1204 23:12:50.416998   17208 fix.go:138] unexpected machine state, will restart: <nil>
I1204 23:12:50.420433   17208 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1204 23:12:50.431840   17208 cli_runner.go:164] Run: docker start minikube
I1204 23:12:50.662932   17208 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 23:12:50.711937   17208 kic.go:430] container "minikube" state is running.
I1204 23:12:50.712499   17208 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1204 23:12:50.763853   17208 profile.go:143] Saving config to /Users/stepan/.minikube/profiles/minikube/config.json ...
I1204 23:12:50.764232   17208 machine.go:93] provisionDockerMachine start ...
I1204 23:12:50.764306   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:50.824251   17208 main.go:141] libmachine: Using SSH client type: native
I1204 23:12:50.824656   17208 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1047254b0] 0x104727cf0 <nil>  [] 0s} 127.0.0.1 62903 <nil> <nil>}
I1204 23:12:50.824664   17208 main.go:141] libmachine: About to run SSH command:
hostname
I1204 23:12:50.829658   17208 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1204 23:12:53.959512   17208 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1204 23:12:53.959535   17208 ubuntu.go:169] provisioning hostname "minikube"
I1204 23:12:53.960110   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:54.025614   17208 main.go:141] libmachine: Using SSH client type: native
I1204 23:12:54.026035   17208 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1047254b0] 0x104727cf0 <nil>  [] 0s} 127.0.0.1 62903 <nil> <nil>}
I1204 23:12:54.026042   17208 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1204 23:12:54.144242   17208 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1204 23:12:54.144435   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:54.202283   17208 main.go:141] libmachine: Using SSH client type: native
I1204 23:12:54.202452   17208 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1047254b0] 0x104727cf0 <nil>  [] 0s} 127.0.0.1 62903 <nil> <nil>}
I1204 23:12:54.202458   17208 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1204 23:12:54.313287   17208 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1204 23:12:54.313306   17208 ubuntu.go:175] set auth options {CertDir:/Users/stepan/.minikube CaCertPath:/Users/stepan/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/stepan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/stepan/.minikube/machines/server.pem ServerKeyPath:/Users/stepan/.minikube/machines/server-key.pem ClientKeyPath:/Users/stepan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/stepan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/stepan/.minikube}
I1204 23:12:54.313337   17208 ubuntu.go:177] setting up certificates
I1204 23:12:54.313346   17208 provision.go:84] configureAuth start
I1204 23:12:54.313528   17208 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1204 23:12:54.377940   17208 provision.go:143] copyHostCerts
I1204 23:12:54.378260   17208 exec_runner.go:144] found /Users/stepan/.minikube/cert.pem, removing ...
I1204 23:12:54.378267   17208 exec_runner.go:203] rm: /Users/stepan/.minikube/cert.pem
I1204 23:12:54.378355   17208 exec_runner.go:151] cp: /Users/stepan/.minikube/certs/cert.pem --> /Users/stepan/.minikube/cert.pem (1119 bytes)
I1204 23:12:54.378677   17208 exec_runner.go:144] found /Users/stepan/.minikube/key.pem, removing ...
I1204 23:12:54.378679   17208 exec_runner.go:203] rm: /Users/stepan/.minikube/key.pem
I1204 23:12:54.378726   17208 exec_runner.go:151] cp: /Users/stepan/.minikube/certs/key.pem --> /Users/stepan/.minikube/key.pem (1679 bytes)
I1204 23:12:54.378993   17208 exec_runner.go:144] found /Users/stepan/.minikube/ca.pem, removing ...
I1204 23:12:54.378996   17208 exec_runner.go:203] rm: /Users/stepan/.minikube/ca.pem
I1204 23:12:54.379043   17208 exec_runner.go:151] cp: /Users/stepan/.minikube/certs/ca.pem --> /Users/stepan/.minikube/ca.pem (1078 bytes)
I1204 23:12:54.379384   17208 provision.go:117] generating server cert: /Users/stepan/.minikube/machines/server.pem ca-key=/Users/stepan/.minikube/certs/ca.pem private-key=/Users/stepan/.minikube/certs/ca-key.pem org=stepan.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1204 23:12:54.495254   17208 provision.go:177] copyRemoteCerts
I1204 23:12:54.495350   17208 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1204 23:12:54.495401   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:54.543387   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:54.624999   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1204 23:12:54.640315   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1204 23:12:54.657052   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1204 23:12:54.674176   17208 provision.go:87] duration metric: took 360.816833ms to configureAuth
I1204 23:12:54.674187   17208 ubuntu.go:193] setting minikube options for container-runtime
I1204 23:12:54.674537   17208 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1204 23:12:54.674612   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:54.726315   17208 main.go:141] libmachine: Using SSH client type: native
I1204 23:12:54.726538   17208 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1047254b0] 0x104727cf0 <nil>  [] 0s} 127.0.0.1 62903 <nil> <nil>}
I1204 23:12:54.726544   17208 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1204 23:12:54.838262   17208 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1204 23:12:54.838272   17208 ubuntu.go:71] root file system type: overlay
I1204 23:12:54.838381   17208 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1204 23:12:54.838491   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:54.888134   17208 main.go:141] libmachine: Using SSH client type: native
I1204 23:12:54.888272   17208 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1047254b0] 0x104727cf0 <nil>  [] 0s} 127.0.0.1 62903 <nil> <nil>}
I1204 23:12:54.888313   17208 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1204 23:12:54.999758   17208 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1204 23:12:54.999975   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:55.055068   17208 main.go:141] libmachine: Using SSH client type: native
I1204 23:12:55.055229   17208 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1047254b0] 0x104727cf0 <nil>  [] 0s} 127.0.0.1 62903 <nil> <nil>}
I1204 23:12:55.055257   17208 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1204 23:12:55.162868   17208 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1204 23:12:55.162885   17208 machine.go:96] duration metric: took 4.398631875s to provisionDockerMachine
I1204 23:12:55.162896   17208 start.go:293] postStartSetup for "minikube" (driver="docker")
I1204 23:12:55.162907   17208 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1204 23:12:55.163115   17208 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1204 23:12:55.163205   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:55.221207   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:55.302228   17208 ssh_runner.go:195] Run: cat /etc/os-release
I1204 23:12:55.310544   17208 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1204 23:12:55.310588   17208 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1204 23:12:55.310606   17208 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1204 23:12:55.310613   17208 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1204 23:12:55.310624   17208 filesync.go:126] Scanning /Users/stepan/.minikube/addons for local assets ...
I1204 23:12:55.310850   17208 filesync.go:126] Scanning /Users/stepan/.minikube/files for local assets ...
I1204 23:12:55.310912   17208 start.go:296] duration metric: took 148.009333ms for postStartSetup
I1204 23:12:55.311038   17208 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1204 23:12:55.311126   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:55.359548   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:55.439808   17208 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1204 23:12:55.442587   17208 fix.go:56] duration metric: took 5.08773975s for fixHost
I1204 23:12:55.442605   17208 start.go:83] releasing machines lock for "minikube", held for 5.087794125s
I1204 23:12:55.442730   17208 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1204 23:12:55.505579   17208 ssh_runner.go:195] Run: cat /version.json
I1204 23:12:55.505677   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:55.507348   17208 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1204 23:12:55.507462   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:55.568888   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:55.569026   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:55.649932   17208 ssh_runner.go:195] Run: systemctl --version
I1204 23:12:55.829719   17208 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1204 23:12:55.835514   17208 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1204 23:12:55.853847   17208 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1204 23:12:55.854249   17208 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1204 23:12:55.862268   17208 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1204 23:12:55.862304   17208 start.go:495] detecting cgroup driver to use...
I1204 23:12:55.862342   17208 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1204 23:12:55.864087   17208 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1204 23:12:55.879100   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1204 23:12:55.887661   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1204 23:12:55.894165   17208 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1204 23:12:55.894363   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1204 23:12:55.899914   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1204 23:12:55.905109   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1204 23:12:55.910183   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1204 23:12:55.915436   17208 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1204 23:12:55.920747   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1204 23:12:55.926024   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1204 23:12:55.931391   17208 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1204 23:12:55.937782   17208 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1204 23:12:55.942636   17208 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1204 23:12:55.947369   17208 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 23:12:55.984920   17208 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1204 23:12:56.038663   17208 start.go:495] detecting cgroup driver to use...
I1204 23:12:56.038688   17208 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1204 23:12:56.038863   17208 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1204 23:12:56.052697   17208 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1204 23:12:56.052887   17208 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1204 23:12:56.062080   17208 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1204 23:12:56.071580   17208 ssh_runner.go:195] Run: which cri-dockerd
I1204 23:12:56.074430   17208 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1204 23:12:56.079035   17208 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1204 23:12:56.088627   17208 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1204 23:12:56.126945   17208 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1204 23:12:56.163229   17208 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1204 23:12:56.163396   17208 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1204 23:12:56.173000   17208 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 23:12:56.210042   17208 ssh_runner.go:195] Run: sudo systemctl restart docker
I1204 23:12:56.482887   17208 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1204 23:12:56.489612   17208 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1204 23:12:56.497684   17208 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1204 23:12:56.503954   17208 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1204 23:12:56.539926   17208 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1204 23:12:56.575946   17208 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 23:12:56.611836   17208 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1204 23:12:56.631692   17208 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1204 23:12:56.638438   17208 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 23:12:56.672799   17208 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1204 23:12:56.728198   17208 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1204 23:12:56.728994   17208 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1204 23:12:56.731727   17208 start.go:563] Will wait 60s for crictl version
I1204 23:12:56.731871   17208 ssh_runner.go:195] Run: which crictl
I1204 23:12:56.734288   17208 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1204 23:12:56.755161   17208 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1204 23:12:56.755394   17208 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1204 23:12:56.769616   17208 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1204 23:12:56.794206   17208 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1204 23:12:56.794824   17208 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1204 23:12:56.939620   17208 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1204 23:12:56.940014   17208 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1204 23:12:56.943076   17208 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1204 23:12:56.949387   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1204 23:12:56.999122   17208 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1204 23:12:56.999244   17208 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1204 23:12:56.999323   17208 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1204 23:12:57.010349   17208 docker.go:685] Got preloaded images: -- stdout --
flask-app:latest
python:bullseye
redis:alpine
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1204 23:12:57.010359   17208 docker.go:615] Images already preloaded, skipping extraction
I1204 23:12:57.010451   17208 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1204 23:12:57.021099   17208 docker.go:685] Got preloaded images: -- stdout --
flask-app:latest
python:bullseye
redis:alpine
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1204 23:12:57.021119   17208 cache_images.go:84] Images are preloaded, skipping loading
I1204 23:12:57.021131   17208 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1204 23:12:57.021231   17208 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1204 23:12:57.021310   17208 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1204 23:12:57.055576   17208 cni.go:84] Creating CNI manager for ""
I1204 23:12:57.055587   17208 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1204 23:12:57.055593   17208 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1204 23:12:57.055608   17208 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1204 23:12:57.055709   17208 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1204 23:12:57.055809   17208 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1204 23:12:57.060997   17208 binaries.go:44] Found k8s binaries, skipping transfer
I1204 23:12:57.061073   17208 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1204 23:12:57.065214   17208 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1204 23:12:57.073619   17208 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1204 23:12:57.082135   17208 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1204 23:12:57.090848   17208 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1204 23:12:57.093254   17208 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1204 23:12:57.098504   17208 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 23:12:57.133204   17208 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1204 23:12:57.152851   17208 certs.go:68] Setting up /Users/stepan/.minikube/profiles/minikube for IP: 192.168.49.2
I1204 23:12:57.152871   17208 certs.go:194] generating shared ca certs ...
I1204 23:12:57.153698   17208 certs.go:226] acquiring lock for ca certs: {Name:mkf3bc1c8a90ad1b57e6e8042d22897f227f4f28 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 23:12:57.154346   17208 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/stepan/.minikube/ca.key
I1204 23:12:57.154784   17208 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/stepan/.minikube/proxy-client-ca.key
I1204 23:12:57.154795   17208 certs.go:256] generating profile certs ...
I1204 23:12:57.155241   17208 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/stepan/.minikube/profiles/minikube/client.key
I1204 23:12:57.155565   17208 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/stepan/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1204 23:12:57.155825   17208 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/stepan/.minikube/profiles/minikube/proxy-client.key
I1204 23:12:57.156196   17208 certs.go:484] found cert: /Users/stepan/.minikube/certs/ca-key.pem (1675 bytes)
I1204 23:12:57.156253   17208 certs.go:484] found cert: /Users/stepan/.minikube/certs/ca.pem (1078 bytes)
I1204 23:12:57.156310   17208 certs.go:484] found cert: /Users/stepan/.minikube/certs/cert.pem (1119 bytes)
I1204 23:12:57.156365   17208 certs.go:484] found cert: /Users/stepan/.minikube/certs/key.pem (1679 bytes)
I1204 23:12:57.163083   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1204 23:12:57.176134   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1204 23:12:57.189341   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1204 23:12:57.202383   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1204 23:12:57.216432   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1204 23:12:57.230685   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1204 23:12:57.281581   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1204 23:12:57.294198   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1204 23:12:57.307484   17208 ssh_runner.go:362] scp /Users/stepan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1204 23:12:57.318706   17208 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1204 23:12:57.327474   17208 ssh_runner.go:195] Run: openssl version
I1204 23:12:57.331257   17208 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1204 23:12:57.339737   17208 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1204 23:12:57.341891   17208 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec  4 19:24 /usr/share/ca-certificates/minikubeCA.pem
I1204 23:12:57.341981   17208 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1204 23:12:57.345551   17208 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1204 23:12:57.350216   17208 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1204 23:12:57.352629   17208 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1204 23:12:57.356314   17208 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1204 23:12:57.360900   17208 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1204 23:12:57.364855   17208 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1204 23:12:57.371512   17208 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1204 23:12:57.375436   17208 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1204 23:12:57.379216   17208 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1204 23:12:57.379393   17208 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1204 23:12:57.388972   17208 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1204 23:12:57.393674   17208 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1204 23:12:57.393680   17208 kubeadm.go:593] restartPrimaryControlPlane start ...
I1204 23:12:57.393740   17208 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1204 23:12:57.398390   17208 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1204 23:12:57.398697   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1204 23:12:57.462112   17208 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /Users/stepan/.kube/config
I1204 23:12:57.462205   17208 kubeconfig.go:62] /Users/stepan/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I1204 23:12:57.462431   17208 lock.go:35] WriteFile acquiring /Users/stepan/.kube/config: {Name:mkf8bf757f2e018127fe97ce8d49fa45901a7687 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 23:12:57.464082   17208 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1204 23:12:57.470000   17208 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1204 23:12:57.470011   17208 kubeadm.go:597] duration metric: took 76.328583ms to restartPrimaryControlPlane
I1204 23:12:57.470015   17208 kubeadm.go:394] duration metric: took 90.805708ms to StartCluster
I1204 23:12:57.470026   17208 settings.go:142] acquiring lock: {Name:mk4c1c51b652cebbc912efbf7b46b59af0fd4b07 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 23:12:57.470100   17208 settings.go:150] Updating kubeconfig:  /Users/stepan/.kube/config
I1204 23:12:57.470461   17208 lock.go:35] WriteFile acquiring /Users/stepan/.kube/config: {Name:mkf8bf757f2e018127fe97ce8d49fa45901a7687 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 23:12:57.470814   17208 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1204 23:12:57.470846   17208 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1204 23:12:57.471037   17208 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1204 23:12:57.471089   17208 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1204 23:12:57.471105   17208 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1204 23:12:57.471109   17208 addons.go:243] addon storage-provisioner should already be in state true
I1204 23:12:57.471119   17208 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1204 23:12:57.471127   17208 host.go:66] Checking if "minikube" exists ...
I1204 23:12:57.471141   17208 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1204 23:12:57.471478   17208 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 23:12:57.471554   17208 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 23:12:57.483189   17208 out.go:177] 🔎  Verifying Kubernetes components...
I1204 23:12:57.487293   17208 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 23:12:57.522252   17208 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1204 23:12:57.522258   17208 addons.go:243] addon default-storageclass should already be in state true
I1204 23:12:57.522270   17208 host.go:66] Checking if "minikube" exists ...
I1204 23:12:57.522518   17208 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 23:12:57.526393   17208 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1204 23:12:57.529972   17208 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1204 23:12:57.530270   17208 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1204 23:12:57.530274   17208 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1204 23:12:57.530322   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:57.537461   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1204 23:12:57.567472   17208 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1204 23:12:57.567489   17208 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1204 23:12:57.567573   17208 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 23:12:57.574656   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:57.582410   17208 api_server.go:52] waiting for apiserver process to appear ...
I1204 23:12:57.582525   17208 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1204 23:12:57.610961   17208 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62903 SSHKeyPath:/Users/stepan/.minikube/machines/minikube/id_rsa Username:docker}
I1204 23:12:57.659204   17208 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1204 23:12:57.694861   17208 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1204 23:12:57.820348   17208 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 23:12:57.820395   17208 retry.go:31] will retry after 300.573635ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1204 23:12:57.820412   17208 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 23:12:57.820420   17208 retry.go:31] will retry after 253.719286ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 23:12:58.074979   17208 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1204 23:12:58.083092   17208 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1204 23:12:58.122070   17208 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1204 23:13:00.096138   17208 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.021107375s)
I1204 23:13:00.096164   17208 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.013049042s)
I1204 23:13:00.096180   17208 api_server.go:72] duration metric: took 2.625314917s to wait for apiserver process to appear ...
I1204 23:13:00.096185   17208 api_server.go:88] waiting for apiserver healthz status ...
I1204 23:13:00.096204   17208 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62907/healthz ...
I1204 23:13:00.104258   17208 api_server.go:279] https://127.0.0.1:62907/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 23:13:00.104276   17208 api_server.go:103] status: https://127.0.0.1:62907/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 23:13:00.345274   17208 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.222795792s)
I1204 23:13:00.354783   17208 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I1204 23:13:00.359272   17208 addons.go:510] duration metric: took 2.888395417s for enable addons: enabled=[default-storageclass storage-provisioner]
I1204 23:13:00.596363   17208 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62907/healthz ...
I1204 23:13:00.602720   17208 api_server.go:279] https://127.0.0.1:62907/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 23:13:00.602738   17208 api_server.go:103] status: https://127.0.0.1:62907/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 23:13:01.097348   17208 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62907/healthz ...
I1204 23:13:01.103725   17208 api_server.go:279] https://127.0.0.1:62907/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 23:13:01.103751   17208 api_server.go:103] status: https://127.0.0.1:62907/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 23:13:01.596552   17208 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62907/healthz ...
I1204 23:13:01.608353   17208 api_server.go:279] https://127.0.0.1:62907/healthz returned 200:
ok
I1204 23:13:01.610613   17208 api_server.go:141] control plane version: v1.31.0
I1204 23:13:01.610627   17208 api_server.go:131] duration metric: took 1.514430542s to wait for apiserver health ...
I1204 23:13:01.611816   17208 system_pods.go:43] waiting for kube-system pods to appear ...
I1204 23:13:01.624092   17208 system_pods.go:59] 7 kube-system pods found
I1204 23:13:01.624148   17208 system_pods.go:61] "coredns-6f6b679f8f-q55fp" [9a573082-cd44-4d0e-9673-3c6d767e9e02] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1204 23:13:01.624159   17208 system_pods.go:61] "etcd-minikube" [acbb08d5-2a9c-48a9-8709-4d04d53aafd2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1204 23:13:01.624168   17208 system_pods.go:61] "kube-apiserver-minikube" [858726a8-a3d6-4b21-b4c3-1f5f45e07b3c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1204 23:13:01.624176   17208 system_pods.go:61] "kube-controller-manager-minikube" [4d839204-43e3-4f73-b4b6-a086bffa1082] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1204 23:13:01.624180   17208 system_pods.go:61] "kube-proxy-2bzf7" [a8b579b3-ee71-46e3-aea6-3c26407898d0] Running
I1204 23:13:01.624185   17208 system_pods.go:61] "kube-scheduler-minikube" [9554d4b6-68eb-4cc6-a4f6-63d8bdc35a7d] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1204 23:13:01.624189   17208 system_pods.go:61] "storage-provisioner" [2db3a54d-c960-4117-9923-da85335324b9] Running
I1204 23:13:01.624199   17208 system_pods.go:74] duration metric: took 12.375166ms to wait for pod list to return data ...
I1204 23:13:01.624215   17208 kubeadm.go:582] duration metric: took 4.153340709s to wait for: map[apiserver:true system_pods:true]
I1204 23:13:01.624238   17208 node_conditions.go:102] verifying NodePressure condition ...
I1204 23:13:01.631320   17208 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1204 23:13:01.631350   17208 node_conditions.go:123] node cpu capacity is 8
I1204 23:13:01.631375   17208 node_conditions.go:105] duration metric: took 7.130583ms to run NodePressure ...
I1204 23:13:01.631393   17208 start.go:241] waiting for startup goroutines ...
I1204 23:13:01.631401   17208 start.go:246] waiting for cluster config update ...
I1204 23:13:01.631417   17208 start.go:255] writing updated cluster config ...
I1204 23:13:01.632565   17208 ssh_runner.go:195] Run: rm -f paused
I1204 23:13:01.838778   17208 start.go:600] kubectl: 1.31.3, cluster: 1.31.0 (minor skew: 0)
I1204 23:13:01.845647   17208 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 04 20:12:56 minikube systemd[1]: Starting Docker Application Container Engine...
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.073582175Z" level=info msg="Starting up"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.179361717Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.211451717Z" level=info msg="Loading containers: start."
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.216102133Z" level=info msg="Processing signal 'terminated'"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.277054092Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.292982758Z" level=info msg="Loading containers: done."
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.299563300Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.299611425Z" level=info msg="Daemon has completed initialization"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.320370300Z" level=info msg="API listen on /var/run/docker.sock"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.320387508Z" level=info msg="API listen on [::]:2376"
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.320987217Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Dec 04 20:12:56 minikube dockerd[659]: time="2024-12-04T20:12:56.321213467Z" level=info msg="Daemon shutdown complete"
Dec 04 20:12:56 minikube systemd[1]: docker.service: Deactivated successfully.
Dec 04 20:12:56 minikube systemd[1]: Stopped Docker Application Container Engine.
Dec 04 20:12:56 minikube systemd[1]: Starting Docker Application Container Engine...
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.352434508Z" level=info msg="Starting up"
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.361686717Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.373251133Z" level=info msg="Loading containers: start."
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.436319800Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.453072592Z" level=info msg="Loading containers: done."
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.459649259Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.459691300Z" level=info msg="Daemon has completed initialization"
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.480492134Z" level=info msg="API listen on /var/run/docker.sock"
Dec 04 20:12:56 minikube dockerd[938]: time="2024-12-04T20:12:56.480494134Z" level=info msg="API listen on [::]:2376"
Dec 04 20:12:56 minikube systemd[1]: Started Docker Application Container Engine.
Dec 04 20:12:56 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Start docker client with request timeout 0s"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Loaded network plugin cni"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 04 20:12:56 minikube cri-dockerd[1219]: time="2024-12-04T20:12:56Z" level=info msg="Start cri-dockerd grpc backend"
Dec 04 20:12:56 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"flask-deployment-f7b77cf85-j98q5_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fe255c895332754e37f670635a0fdbcd1df9f95f4aa46b20d4db12e50775d72d\""
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-q55fp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5307bdc7c83e8c41277d3be287c7f988928c7698d4eeae26dffbbe0cb0f27340\""
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"test-pod_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"decc19c48b74b7886eca8d145dbc4b519d2cf8d51bcd7da68af9d8a6edc5558a\""
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"redis-deployment-6664db5d7c-qcg2v_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f2ff6cc22d415f6a834a18b68f649b4fa4112163650c8f97c7a0c0c111d305c8\""
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4dfbc144ac7cd09d2e72391c669043631f2be7ae0f76755f4f1b7bf823e3760/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a572c1c925bf0433f53598fd0ab732808eabdfd03ad99e4f493629286b2de850/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a42e7c21336c2f9293be5409ac8c989db87898cdf8837506d776d91a9cd4994/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:12:57 minikube cri-dockerd[1219]: time="2024-12-04T20:12:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d92c2310072fe229671f9cc3f10d70b1209762385281e9f49215e7e3081fcec3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc7f9eadb6c16db650a0ecdd626d11bee7694a898f1d1a83646cc229d51b9780/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6a61efdebac2d8d4e01f39f3b86899cb11895470e4ba6463b886e05dee598b8c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f75cad6ef10bfe7cd663822fc8dd8e2dfec21bbe9287ca344a574266c4b0f5e3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bd4eaabd0a61671ef128cc31488fddce7e38a95333c6c3e973abc3ed16dab4a2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9443c640c22e444cfba525dcc3ad9db8713f99a773ec513ec1ad8fa4339adcf0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 20:13:00 minikube cri-dockerd[1219]: time="2024-12-04T20:13:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98b2a299e1dcaddcd17e61e4b82d1590e1a419a21f988e44eb7b6074ff86a0ab/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 20:13:02 minikube dockerd[938]: time="2024-12-04T20:13:02.920412262Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 04 20:13:02 minikube dockerd[938]: time="2024-12-04T20:13:02.920479220Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 04 20:13:16 minikube dockerd[938]: time="2024-12-04T20:13:16.024971670Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 04 20:13:16 minikube dockerd[938]: time="2024-12-04T20:13:16.025131837Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 04 20:13:30 minikube dockerd[938]: time="2024-12-04T20:13:30.910753927Z" level=info msg="ignoring event" container=ef0231599d5a9172261dce2ef02e0122e413516d7c39bbadd7b058471121a4b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 04 20:13:41 minikube dockerd[938]: time="2024-12-04T20:13:41.611436710Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 04 20:13:41 minikube dockerd[938]: time="2024-12-04T20:13:41.611714043Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                            CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
49e6089b43a0e       ba04bb24b9575                                                                    31 seconds ago       Running             storage-provisioner       3                   6a61efdebac2d       storage-provisioner
245a3cb65773e       8a500e4943368                                                                    About a minute ago   Running             python-container          1                   98b2a299e1dca       test-pod
3e945255435d1       4a6fa001a1b08                                                                    About a minute ago   Running             redis                     1                   bd4eaabd0a616       redis-deployment-6664db5d7c-qcg2v
5c7cda5e68462       2437cf7621777                                                                    About a minute ago   Running             coredns                   1                   f75cad6ef10bf       coredns-6f6b679f8f-q55fp
ef0231599d5a9       ba04bb24b9575                                                                    About a minute ago   Exited              storage-provisioner       2                   6a61efdebac2d       storage-provisioner
f5eabc94c5b3f       71d55d66fd4ee                                                                    About a minute ago   Running             kube-proxy                1                   cc7f9eadb6c16       kube-proxy-2bzf7
ae4938bcc8177       cd0f0ae0ec9e0                                                                    About a minute ago   Running             kube-apiserver            1                   5a42e7c21336c       kube-apiserver-minikube
4702e614c219d       fbbbd428abb4d                                                                    About a minute ago   Running             kube-scheduler            1                   d92c2310072fe       kube-scheduler-minikube
c89858b947c1d       fcb0683e6bdbd                                                                    About a minute ago   Running             kube-controller-manager   1                   a572c1c925bf0       kube-controller-manager-minikube
91c8940c061d7       27e3830e14027                                                                    About a minute ago   Running             etcd                      1                   a4dfbc144ac7c       etcd-minikube
5779400365d73       redis@sha256:c1e88455c85225310bbea54816e9c3f4b5295815e6dbf80c34d40afc6df28275    20 minutes ago       Exited              redis                     0                   f2ff6cc22d415       redis-deployment-6664db5d7c-qcg2v
3ac67adb066b2       python@sha256:1e7bbc47b5e0b069a90ad1a85985fbda130a54a3a241ac9269166f8a49ba3053   38 minutes ago       Exited              python-container          0                   decc19c48b74b       test-pod
6f802277f1a84       71d55d66fd4ee                                                                    49 minutes ago       Exited              kube-proxy                0                   0daecee6a1edc       kube-proxy-2bzf7
edba9bc3365d3       2437cf7621777                                                                    49 minutes ago       Exited              coredns                   0                   5307bdc7c83e8       coredns-6f6b679f8f-q55fp
42e59737681df       fbbbd428abb4d                                                                    49 minutes ago       Exited              kube-scheduler            0                   cc7d9d6f414f2       kube-scheduler-minikube
8ce083768c408       fcb0683e6bdbd                                                                    49 minutes ago       Exited              kube-controller-manager   0                   5e6b94fc4cf6a       kube-controller-manager-minikube
d6fda490b88d9       27e3830e14027                                                                    49 minutes ago       Exited              etcd                      0                   a5551bddfe55d       etcd-minikube
89d5b64af97e9       cd0f0ae0ec9e0                                                                    49 minutes ago       Exited              kube-apiserver            0                   ecbc630e68e98       kube-apiserver-minikube


==> coredns [5c7cda5e6846] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:60509 - 53014 "HINFO IN 1742139709570496844.2664290634049136754. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.049465458s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[737133825]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Dec-2024 20:13:01.006) (total time: 30002ms):
Trace[737133825]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (20:13:31.008)
Trace[737133825]: [30.002256222s] [30.002256222s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1087734602]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Dec-2024 20:13:01.006) (total time: 30002ms):
Trace[1087734602]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (20:13:31.008)
Trace[1087734602]: [30.002296389s] [30.002296389s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[611159546]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Dec-2024 20:13:01.006) (total time: 30002ms):
Trace[611159546]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (20:13:31.008)
Trace[611159546]: [30.00282043s] [30.00282043s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [edba9bc3365d] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:43245 - 48105 "HINFO IN 488867534882266498.1826780243981915366. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.034063542s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1474130800]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Dec-2024 19:24:38.570) (total time: 30003ms):
Trace[1474130800]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (19:25:08.571)
Trace[1474130800]: [30.003532681s] [30.003532681s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1047133344]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Dec-2024 19:24:38.570) (total time: 30004ms):
Trace[1047133344]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (19:25:08.573)
Trace[1047133344]: [30.004225931s] [30.004225931s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1564320363]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Dec-2024 19:24:38.570) (total time: 30004ms):
Trace[1564320363]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (19:25:08.573)
Trace[1564320363]: [30.004387722s] [30.004387722s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_12_04T22_24_33_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 04 Dec 2024 19:24:30 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 04 Dec 2024 20:14:11 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 04 Dec 2024 20:13:00 +0000   Wed, 04 Dec 2024 19:24:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 04 Dec 2024 20:13:00 +0000   Wed, 04 Dec 2024 19:24:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 04 Dec 2024 20:13:00 +0000   Wed, 04 Dec 2024 19:24:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 04 Dec 2024 20:13:00 +0000   Wed, 04 Dec 2024 19:24:30 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8029496Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8029496Ki
  pods:               110
System Info:
  Machine ID:                 e86a25cdbd3144ecb65302feddbabae0
  System UUID:                e86a25cdbd3144ecb65302feddbabae0
  Boot ID:                    4673a7a0-b467-49a6-a4d7-0267db0e696a
  Kernel Version:             6.6.16-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     flask-deployment-f7b77cf85-j98q5     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m4s
  default                     redis-deployment-6664db5d7c-qcg2v    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m
  default                     test-pod                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m
  kube-system                 coredns-6f6b679f8f-q55fp             100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     49m
  kube-system                 etcd-minikube                        100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         49m
  kube-system                 kube-apiserver-minikube              250m (3%)     0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 kube-controller-manager-minikube     200m (2%)     0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 kube-proxy-2bzf7                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 kube-scheduler-minikube              100m (1%)     0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 storage-provisioner                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 49m                kube-proxy       
  Normal  Starting                 75s                kube-proxy       
  Normal  NodeHasSufficientPID     49m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  49m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  49m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    49m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 49m                kubelet          Starting kubelet.
  Normal  RegisteredNode           49m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 79s                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  79s (x8 over 79s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    79s (x8 over 79s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     79s (x7 over 79s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  79s                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           73s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec 4 11:59] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.207666] netlink: 'init': attribute type 4 has an invalid length.
[  +0.007632] fakeowner: loading out-of-tree module taints kernel.
[Dec 4 12:50] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000003] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.001040] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000002] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Dec 4 19:24] systemd[22270]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set


==> etcd [91c8940c061d] <==
{"level":"warn","ts":"2024-12-04T20:12:57.920100Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-12-04T20:12:57.920220Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-12-04T20:12:57.920280Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-12-04T20:12:57.920301Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-12-04T20:12:57.920309Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-12-04T20:12:57.920328Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-12-04T20:12:57.921002Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-12-04T20:12:57.921114Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"arm64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-12-04T20:12:57.974596Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"53.357917ms"}
{"level":"info","ts":"2024-12-04T20:12:57.987125Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-12-04T20:12:57.998984Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":3524}
{"level":"info","ts":"2024-12-04T20:12:57.999277Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-12-04T20:12:57.999351Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2024-12-04T20:12:57.999377Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 3524, applied: 0, lastindex: 3524, lastterm: 2]"}
{"level":"warn","ts":"2024-12-04T20:12:58.002114Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-12-04T20:12:58.002540Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":2473}
{"level":"info","ts":"2024-12-04T20:12:58.003678Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":2918}
{"level":"info","ts":"2024-12-04T20:12:58.004290Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-12-04T20:12:58.005203Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-12-04T20:12:58.005447Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-12-04T20:12:58.005481Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-12-04T20:12:58.005807Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-04T20:12:58.006682Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-12-04T20:12:58.006836Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-12-04T20:12:58.006848Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-12-04T20:12:58.006951Z","caller":"etcdserver/server.go:767","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-12-04T20:12:58.007076Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-04T20:12:58.007112Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-04T20:12:58.007116Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-04T20:12:58.007291Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-04T20:12:58.007297Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-04T20:12:58.008007Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-12-04T20:12:58.008070Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-12-04T20:12:58.008150Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-04T20:12:58.008177Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-04T20:12:59.402633Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-12-04T20:12:59.402875Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-12-04T20:12:59.402925Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-12-04T20:12:59.402956Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-12-04T20:12:59.402969Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-12-04T20:12:59.403048Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-12-04T20:12:59.403069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-12-04T20:12:59.421995Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-12-04T20:12:59.421985Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-04T20:12:59.422282Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-04T20:12:59.422807Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-12-04T20:12:59.422936Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-04T20:12:59.424413Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-04T20:12:59.424804Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-04T20:12:59.426548Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-12-04T20:12:59.426559Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> etcd [d6fda490b88d] <==
{"level":"info","ts":"2024-12-04T19:24:29.032379Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-04T19:24:29.032402Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-04T19:24:29.324197Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-12-04T19:24:29.324255Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-12-04T19:24:29.324302Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-12-04T19:24:29.324320Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-12-04T19:24:29.324323Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-12-04T19:24:29.324329Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-12-04T19:24:29.324334Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-12-04T19:24:29.325258Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-04T19:24:29.325701Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-04T19:24:29.325740Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-04T19:24:29.325750Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-04T19:24:29.325772Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-12-04T19:24:29.325913Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-04T19:24:29.326150Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-04T19:24:29.326718Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-04T19:24:29.326736Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-04T19:24:29.327513Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-12-04T19:24:29.327518Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-12-04T19:24:29.327731Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-12-04T19:24:29.327777Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-04T19:34:29.359493Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":603}
{"level":"info","ts":"2024-12-04T19:34:29.366586Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":603,"took":"6.095834ms","hash":4129659461,"current-db-size-bytes":1355776,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1355776,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-12-04T19:34:29.366701Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4129659461,"revision":603,"compact-revision":-1}
{"level":"info","ts":"2024-12-04T19:36:13.241223Z","caller":"traceutil/trace.go:171","msg":"trace[1014384719] transaction","detail":"{read_only:false; response_revision:930; number_of_response:1; }","duration":"121.562125ms","start":"2024-12-04T19:36:13.119632Z","end":"2024-12-04T19:36:13.241194Z","steps":["trace[1014384719] 'process raft request'  (duration: 121.487167ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T19:39:29.375877Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":841}
{"level":"info","ts":"2024-12-04T19:39:29.382971Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":841,"took":"5.835167ms","hash":1137880596,"current-db-size-bytes":1355776,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":929792,"current-db-size-in-use":"930 kB"}
{"level":"info","ts":"2024-12-04T19:39:29.383131Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1137880596,"revision":841,"compact-revision":603}
{"level":"info","ts":"2024-12-04T19:44:29.390898Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1090}
{"level":"info","ts":"2024-12-04T19:44:29.398953Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1090,"took":"6.60975ms","hash":811814299,"current-db-size-bytes":1355776,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":929792,"current-db-size-in-use":"930 kB"}
{"level":"info","ts":"2024-12-04T19:44:29.399062Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":811814299,"revision":1090,"compact-revision":841}
{"level":"info","ts":"2024-12-04T19:49:29.422603Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1330}
{"level":"info","ts":"2024-12-04T19:49:29.429751Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1330,"took":"6.030125ms","hash":4066723384,"current-db-size-bytes":1355776,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":888832,"current-db-size-in-use":"889 kB"}
{"level":"info","ts":"2024-12-04T19:49:29.429850Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4066723384,"revision":1330,"compact-revision":1090}
{"level":"info","ts":"2024-12-04T19:54:29.440667Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1568}
{"level":"info","ts":"2024-12-04T19:54:29.460820Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1568,"took":"19.22125ms","hash":733331837,"current-db-size-bytes":1355776,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1019904,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-12-04T19:54:29.460933Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":733331837,"revision":1568,"compact-revision":1330}
{"level":"info","ts":"2024-12-04T19:59:29.468051Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1852}
{"level":"info","ts":"2024-12-04T19:59:29.493030Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1852,"took":"19.424041ms","hash":3221767779,"current-db-size-bytes":1454080,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1327104,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-12-04T19:59:29.493129Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3221767779,"revision":1852,"compact-revision":1568}
{"level":"info","ts":"2024-12-04T20:00:26.642015Z","caller":"traceutil/trace.go:171","msg":"trace[1366335297] transaction","detail":"{read_only:false; response_revision:2224; number_of_response:1; }","duration":"102.607125ms","start":"2024-12-04T20:00:26.539373Z","end":"2024-12-04T20:00:26.641980Z","steps":["trace[1366335297] 'process raft request'  (duration: 102.504083ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T20:04:29.482208Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2173}
{"level":"info","ts":"2024-12-04T20:04:29.490159Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2173,"took":"7.316792ms","hash":4267821705,"current-db-size-bytes":1556480,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1544192,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-12-04T20:04:29.490210Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4267821705,"revision":2173,"compact-revision":1852}
{"level":"info","ts":"2024-12-04T20:09:29.494682Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2473}
{"level":"info","ts":"2024-12-04T20:09:29.498803Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2473,"took":"3.359ms","hash":233562390,"current-db-size-bytes":1556480,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1277952,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-12-04T20:09:29.498892Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":233562390,"revision":2473,"compact-revision":2173}
{"level":"info","ts":"2024-12-04T20:10:04.870887Z","caller":"traceutil/trace.go:171","msg":"trace[16870471] transaction","detail":"{read_only:false; response_revision:2746; number_of_response:1; }","duration":"108.606042ms","start":"2024-12-04T20:10:04.762269Z","end":"2024-12-04T20:10:04.870875Z","steps":["trace[16870471] 'process raft request'  (duration: 108.486251ms)"],"step_count":1}
{"level":"info","ts":"2024-12-04T20:12:35.523041Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-12-04T20:12:35.523216Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-12-04T20:12:35.523324Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-12-04T20:12:35.523416Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-12-04T20:12:35.523546Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2024-12-04T20:12:35.544239Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-12-04T20:12:35.544281Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-12-04T20:12:35.544330Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-12-04T20:12:35.545547Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-04T20:12:35.545631Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-04T20:12:35.545642Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 20:14:16 up  8:14,  0 users,  load average: 2.93, 2.87, 2.85
Linux minikube 6.6.16-linuxkit #1 SMP Fri Feb 16 11:54:02 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [89d5b64af97e] <==
I1204 20:12:35.534756       1 controller.go:176] quota evaluator worker shutdown
I1204 20:12:35.534760       1 controller.go:176] quota evaluator worker shutdown
I1204 20:12:35.535836       1 dynamic_cafile_content.go:174] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
W1204 20:12:35.536449       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.530480       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.530493       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532336       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532430       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532491       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532713       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532795       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532857       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532921       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.532978       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533008       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533042       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533075       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533092       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533167       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533185       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533233       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533247       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533326       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533389       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533459       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533503       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533539       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.533505       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534078       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534169       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534264       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534334       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534380       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534438       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534486       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534559       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534615       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534661       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534706       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534886       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.534943       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.536319       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.536401       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.536471       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.536488       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.536565       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.536575       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538118       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538174       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538840       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538893       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538937       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538940       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.538977       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.539007       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.539026       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.539053       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.539187       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.539337       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1204 20:12:36.539543       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [ae4938bcc817] <==
W1204 20:12:59.714907       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1204 20:12:59.715212       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1204 20:12:59.715226       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1204 20:12:59.720782       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1204 20:12:59.720795       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1204 20:12:59.976537       1 secure_serving.go:213] Serving securely on [::]:8443
I1204 20:12:59.976658       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1204 20:12:59.976750       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1204 20:12:59.976838       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1204 20:12:59.976920       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1204 20:12:59.976941       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1204 20:12:59.976949       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1204 20:12:59.976977       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1204 20:12:59.976988       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1204 20:12:59.977002       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1204 20:12:59.977028       1 controller.go:78] Starting OpenAPI AggregationController
I1204 20:12:59.977127       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1204 20:12:59.977221       1 controller.go:142] Starting OpenAPI controller
I1204 20:12:59.977283       1 controller.go:90] Starting OpenAPI V3 controller
I1204 20:12:59.977328       1 naming_controller.go:294] Starting NamingConditionController
I1204 20:12:59.977346       1 crd_finalizer.go:269] Starting CRDFinalizer
I1204 20:12:59.977237       1 controller.go:119] Starting legacy_token_tracking_controller
I1204 20:12:59.977358       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1204 20:12:59.977371       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1204 20:12:59.977397       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1204 20:12:59.977473       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1204 20:12:59.977401       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1204 20:12:59.977306       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1204 20:12:59.977333       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1204 20:12:59.977359       1 establishing_controller.go:81] Starting EstablishingController
I1204 20:12:59.979640       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1204 20:12:59.979710       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1204 20:12:59.980081       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1204 20:12:59.980154       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1204 20:12:59.980183       1 local_available_controller.go:156] Starting LocalAvailability controller
I1204 20:12:59.980192       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1204 20:12:59.980211       1 aggregator.go:169] waiting for initial CRD sync...
I1204 20:12:59.980876       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1204 20:12:59.980890       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1204 20:13:00.076989       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1204 20:13:00.077014       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1204 20:13:00.077264       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1204 20:13:00.077287       1 policy_source.go:224] refreshing policies
I1204 20:13:00.077326       1 shared_informer.go:320] Caches are synced for node_authorizer
I1204 20:13:00.077473       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1204 20:13:00.077689       1 shared_informer.go:320] Caches are synced for configmaps
I1204 20:13:00.078155       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1204 20:13:00.078245       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1204 20:13:00.080499       1 cache.go:39] Caches are synced for LocalAvailability controller
I1204 20:13:00.080913       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1204 20:13:00.081058       1 aggregator.go:171] initial CRD sync complete...
I1204 20:13:00.081091       1 autoregister_controller.go:144] Starting autoregister controller
I1204 20:13:00.081106       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1204 20:13:00.081128       1 cache.go:39] Caches are synced for autoregister controller
I1204 20:13:00.081280       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1204 20:13:00.093083       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1204 20:13:00.288864       1 controller.go:615] quota admission added evaluator for: endpoints
I1204 20:13:00.984032       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W1204 20:13:01.185627       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1204 20:13:01.189225       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-controller-manager [8ce083768c40] <==
I1204 19:55:17.163169       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="51.083µs"
I1204 19:55:31.167487       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="51.834µs"
I1204 19:56:09.147270       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="78.75µs"
I1204 19:56:24.150546       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="73.667µs"
I1204 19:57:32.162843       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="317.584µs"
I1204 19:57:46.159571       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="195.375µs"
I1204 19:57:47.050075       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="18.082208ms"
I1204 19:57:47.050205       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="89.375µs"
I1204 19:57:47.055025       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="3.531ms"
I1204 19:57:47.055126       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="61.125µs"
I1204 19:57:47.056957       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="46µs"
I1204 19:57:47.153507       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="89.292µs"
I1204 19:57:47.569026       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="31.25µs"
I1204 19:57:47.570926       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="18.833µs"
I1204 19:57:47.572124       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="19.833µs"
I1204 19:57:50.316460       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="34.125µs"
I1204 19:58:04.152168       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="68.958µs"
I1204 19:58:21.167990       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="39.833µs"
I1204 19:58:32.163338       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="47.209µs"
I1204 19:58:48.160898       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="54.166µs"
I1204 19:58:59.158465       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="69.417µs"
I1204 19:59:15.490291       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1204 19:59:38.160977       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="50.334µs"
I1204 19:59:53.160257       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="52.042µs"
I1204 20:00:37.168233       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1204 20:01:08.177044       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="241.042µs"
I1204 20:01:20.165399       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="307.333µs"
I1204 20:01:38.145839       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1204 20:02:03.749846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="16.712167ms"
I1204 20:02:03.755413       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="5.522959ms"
I1204 20:02:03.755565       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="35.875µs"
I1204 20:02:03.760863       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="26.375µs"
I1204 20:02:03.831218       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="55.583µs"
I1204 20:02:04.461387       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="117.458µs"
I1204 20:02:04.464064       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="30.458µs"
I1204 20:02:04.467216       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="42.416µs"
I1204 20:02:06.511975       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="90.292µs"
I1204 20:02:19.168483       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="47.333µs"
I1204 20:02:35.164794       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="44.083µs"
I1204 20:02:48.165236       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="190.167µs"
I1204 20:03:02.157895       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="60.916µs"
I1204 20:03:13.164373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="59.167µs"
I1204 20:03:53.162629       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="120.708µs"
I1204 20:04:04.164593       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="79.834µs"
I1204 20:05:25.175217       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="166.667µs"
I1204 20:05:40.165469       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="98.084µs"
I1204 20:06:44.010923       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1204 20:08:10.165106       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="80.458µs"
I1204 20:08:25.158609       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="89.083µs"
I1204 20:10:45.935421       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="14.625µs"
I1204 20:11:12.511186       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="12.549792ms"
I1204 20:11:12.519727       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="8.494125ms"
I1204 20:11:12.519770       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="19.834µs"
I1204 20:11:15.917245       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="58.458µs"
I1204 20:11:27.168124       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="154.75µs"
I1204 20:11:44.165783       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="99.125µs"
I1204 20:11:49.459637       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1204 20:11:59.168683       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="129.5µs"
I1204 20:12:11.165490       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="51.792µs"
I1204 20:12:25.167772       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="65.167µs"


==> kube-controller-manager [c89858b947c1] <==
I1204 20:13:03.220262       1 shared_informer.go:320] Caches are synced for job
I1204 20:13:03.220343       1 shared_informer.go:320] Caches are synced for cronjob
I1204 20:13:03.221702       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1204 20:13:03.227401       1 shared_informer.go:320] Caches are synced for HPA
I1204 20:13:03.230916       1 shared_informer.go:320] Caches are synced for TTL after finished
I1204 20:13:03.233079       1 shared_informer.go:320] Caches are synced for ephemeral
I1204 20:13:03.253699       1 shared_informer.go:320] Caches are synced for node
I1204 20:13:03.253807       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1204 20:13:03.253890       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1204 20:13:03.253896       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1204 20:13:03.253903       1 shared_informer.go:320] Caches are synced for cidrallocator
I1204 20:13:03.254038       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1204 20:13:03.254265       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1204 20:13:03.256713       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1204 20:13:03.258024       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1204 20:13:03.258300       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-6664db5d7c" duration="168.167µs"
I1204 20:13:03.258300       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="197.917µs"
I1204 20:13:03.260790       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1204 20:13:03.260986       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1204 20:13:03.260997       1 shared_informer.go:320] Caches are synced for ReplicationController
I1204 20:13:03.261205       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1204 20:13:03.261223       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1204 20:13:03.261253       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1204 20:13:03.264044       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1204 20:13:03.264135       1 shared_informer.go:320] Caches are synced for GC
I1204 20:13:03.265042       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1204 20:13:03.266965       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1204 20:13:03.268936       1 shared_informer.go:320] Caches are synced for PV protection
I1204 20:13:03.269091       1 shared_informer.go:320] Caches are synced for deployment
I1204 20:13:03.269914       1 shared_informer.go:320] Caches are synced for stateful set
I1204 20:13:03.271732       1 shared_informer.go:320] Caches are synced for disruption
I1204 20:13:03.273278       1 shared_informer.go:320] Caches are synced for PVC protection
I1204 20:13:03.301071       1 shared_informer.go:320] Caches are synced for taint
I1204 20:13:03.301185       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1204 20:13:03.301264       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1204 20:13:03.301365       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1204 20:13:03.306112       1 shared_informer.go:320] Caches are synced for persistent volume
I1204 20:13:03.310552       1 shared_informer.go:320] Caches are synced for daemon sets
I1204 20:13:03.312297       1 shared_informer.go:320] Caches are synced for TTL
I1204 20:13:03.358858       1 shared_informer.go:320] Caches are synced for attach detach
I1204 20:13:03.417547       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1204 20:13:03.429088       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="51.084µs"
I1204 20:13:03.452163       1 shared_informer.go:320] Caches are synced for resource quota
I1204 20:13:03.460053       1 shared_informer.go:320] Caches are synced for namespace
I1204 20:13:03.460637       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="202.414292ms"
I1204 20:13:03.460961       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="46.917µs"
I1204 20:13:03.462075       1 shared_informer.go:320] Caches are synced for service account
I1204 20:13:03.464106       1 shared_informer.go:320] Caches are synced for endpoint
I1204 20:13:03.475060       1 shared_informer.go:320] Caches are synced for resource quota
I1204 20:13:03.503720       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1204 20:13:03.906317       1 shared_informer.go:320] Caches are synced for garbage collector
I1204 20:13:03.906413       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1204 20:13:03.964795       1 shared_informer.go:320] Caches are synced for garbage collector
I1204 20:13:14.253129       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="49µs"
I1204 20:13:27.253451       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="36.541µs"
I1204 20:13:35.265627       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="9.939375ms"
I1204 20:13:35.265747       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="53.875µs"
I1204 20:13:39.245454       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="32.958µs"
I1204 20:13:56.260592       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="61.792µs"
I1204 20:14:09.251116       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-f7b77cf85" duration="69.667µs"


==> kube-proxy [6f802277f1a8] <==
I1204 19:24:39.014688       1 server_linux.go:66] "Using iptables proxy"
I1204 19:24:39.086712       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1204 19:24:39.086774       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1204 19:24:39.096858       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1204 19:24:39.096892       1 server_linux.go:169] "Using iptables Proxier"
I1204 19:24:39.097836       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1204 19:24:39.098075       1 server.go:483] "Version info" version="v1.31.0"
I1204 19:24:39.098089       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 19:24:39.099072       1 config.go:197] "Starting service config controller"
I1204 19:24:39.099114       1 shared_informer.go:313] Waiting for caches to sync for service config
I1204 19:24:39.099135       1 config.go:104] "Starting endpoint slice config controller"
I1204 19:24:39.099148       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1204 19:24:39.099506       1 config.go:326] "Starting node config controller"
I1204 19:24:39.099519       1 shared_informer.go:313] Waiting for caches to sync for node config
I1204 19:24:39.199551       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1204 19:24:39.199604       1 shared_informer.go:320] Caches are synced for node config
I1204 19:24:39.199605       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [f5eabc94c5b3] <==
I1204 20:13:00.904880       1 server_linux.go:66] "Using iptables proxy"
I1204 20:13:00.991056       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1204 20:13:00.991169       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1204 20:13:01.008941       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1204 20:13:01.009162       1 server_linux.go:169] "Using iptables Proxier"
I1204 20:13:01.010478       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1204 20:13:01.010784       1 server.go:483] "Version info" version="v1.31.0"
I1204 20:13:01.010800       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 20:13:01.013012       1 config.go:104] "Starting endpoint slice config controller"
I1204 20:13:01.013785       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1204 20:13:01.013751       1 config.go:197] "Starting service config controller"
I1204 20:13:01.013933       1 shared_informer.go:313] Waiting for caches to sync for service config
I1204 20:13:01.013307       1 config.go:326] "Starting node config controller"
I1204 20:13:01.013956       1 shared_informer.go:313] Waiting for caches to sync for node config
I1204 20:13:01.114055       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1204 20:13:01.114902       1 shared_informer.go:320] Caches are synced for node config
I1204 20:13:01.114923       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [42e59737681d] <==
I1204 19:24:29.531337       1 serving.go:386] Generated self-signed cert in-memory
W1204 19:24:30.202152       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1204 19:24:30.202182       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1204 19:24:30.202188       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1204 19:24:30.202193       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1204 19:24:30.211711       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1204 19:24:30.211735       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 19:24:30.213030       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1204 19:24:30.213132       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1204 19:24:30.213165       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1204 19:24:30.213353       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W1204 19:24:30.214023       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1204 19:24:30.214058       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1204 19:24:30.214491       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1204 19:24:30.214511       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214570       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1204 19:24:30.214580       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1204 19:24:30.214589       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1204 19:24:30.214594       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214621       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1204 19:24:30.214623       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1204 19:24:30.214629       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1204 19:24:30.214630       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214616       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1204 19:24:30.214645       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214655       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1204 19:24:30.214661       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214669       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1204 19:24:30.214676       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214684       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1204 19:24:30.214692       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1204 19:24:30.214697       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1204 19:24:30.214706       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1204 19:24:30.214711       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1204 19:24:30.214713       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214719       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1204 19:24:30.214735       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1204 19:24:30.214697       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1204 19:24:30.214696       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1204 19:24:30.214718       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1204 19:24:30.214759       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1204 19:24:31.224775       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1204 19:24:31.224804       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1204 19:24:31.613218       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1204 20:12:35.457711       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I1204 20:12:35.457805       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1204 20:12:35.458036       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1204 20:12:35.458457       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [4702e614c219] <==
I1204 20:12:58.519972       1 serving.go:386] Generated self-signed cert in-memory
I1204 20:13:00.004517       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1204 20:13:00.004543       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 20:13:00.007426       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1204 20:13:00.007529       1 requestheader_controller.go:172] Starting RequestHeaderAuthRequestController
I1204 20:13:00.007579       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I1204 20:13:00.007718       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1204 20:13:00.007903       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1204 20:13:00.007917       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1204 20:13:00.008134       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1204 20:13:00.008138       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1204 20:13:00.107936       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I1204 20:13:00.107976       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1204 20:13:00.108215       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.333725    1445 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f2ff6cc22d415f6a834a18b68f649b4fa4112163650c8f97c7a0c0c111d305c8"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.333728    1445 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="02c612bd71b899847457303c3dce2b9a104cdd158551adf66d2ea5b107f05e2f"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.377544    1445 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Dec 04 20:12:57 minikube kubelet[1445]: E1204 20:12:57.377807    1445 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 04 20:12:57 minikube kubelet[1445]: E1204 20:12:57.420215    1445 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="400ms"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424379    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424409    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424421    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424438    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424451    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/a5363f4f31e043bdae3c93aca4991903-etcd-certs\") pod \"etcd-minikube\" (UID: \"a5363f4f31e043bdae3c93aca4991903\") " pod="kube-system/etcd-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424458    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424468    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424476    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424485    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424492    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424501    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424509    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424520    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/e039200acb850c82bb901653cc38ff6e-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"e039200acb850c82bb901653cc38ff6e\") " pod="kube-system/kube-scheduler-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424529    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/a5363f4f31e043bdae3c93aca4991903-etcd-data\") pod \"etcd-minikube\" (UID: \"a5363f4f31e043bdae3c93aca4991903\") " pod="kube-system/etcd-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.424535    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.578563    1445 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Dec 04 20:12:57 minikube kubelet[1445]: E1204 20:12:57.578858    1445 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 04 20:12:57 minikube kubelet[1445]: E1204 20:12:57.821252    1445 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Dec 04 20:12:57 minikube kubelet[1445]: I1204 20:12:57.979882    1445 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Dec 04 20:12:57 minikube kubelet[1445]: E1204 20:12:57.980231    1445 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 04 20:12:58 minikube kubelet[1445]: I1204 20:12:58.317985    1445 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0ef148258dd3bf4047cbc0c551c5183e5808d6aeb5a5631f6448df62407b16c4"
Dec 04 20:12:58 minikube kubelet[1445]: I1204 20:12:58.781208    1445 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.091342    1445 kubelet_node_status.go:111] "Node was previously registered" node="minikube"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.091428    1445 kubelet_node_status.go:75] "Successfully registered node" node="minikube"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.091444    1445 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.092517    1445 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.216168    1445 apiserver.go:52] "Watching apiserver"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.219301    1445 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.300010    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/a8b579b3-ee71-46e3-aea6-3c26407898d0-lib-modules\") pod \"kube-proxy-2bzf7\" (UID: \"a8b579b3-ee71-46e3-aea6-3c26407898d0\") " pod="kube-system/kube-proxy-2bzf7"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.300079    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/a8b579b3-ee71-46e3-aea6-3c26407898d0-xtables-lock\") pod \"kube-proxy-2bzf7\" (UID: \"a8b579b3-ee71-46e3-aea6-3c26407898d0\") " pod="kube-system/kube-proxy-2bzf7"
Dec 04 20:13:00 minikube kubelet[1445]: I1204 20:13:00.300119    1445 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/2db3a54d-c960-4117-9923-da85335324b9-tmp\") pod \"storage-provisioner\" (UID: \"2db3a54d-c960-4117-9923-da85335324b9\") " pod="kube-system/storage-provisioner"
Dec 04 20:13:00 minikube kubelet[1445]: E1204 20:13:00.341210    1445 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Dec 04 20:13:00 minikube kubelet[1445]: E1204 20:13:00.341245    1445 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 04 20:13:02 minikube kubelet[1445]: I1204 20:13:02.401549    1445 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Dec 04 20:13:02 minikube kubelet[1445]: E1204 20:13:02.925073    1445 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flask-app:latest"
Dec 04 20:13:02 minikube kubelet[1445]: E1204 20:13:02.925176    1445 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flask-app:latest"
Dec 04 20:13:02 minikube kubelet[1445]: E1204 20:13:02.925431    1445 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:flask-app,Image:flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:REDIS_HOST,Value:redis,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brk6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-deployment-f7b77cf85-j98q5_default(6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3): ErrImagePull: Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Dec 04 20:13:02 minikube kubelet[1445]: E1204 20:13:02.927200    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"
Dec 04 20:13:03 minikube kubelet[1445]: E1204 20:13:03.422387    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"flask-app:latest\\\"\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"
Dec 04 20:13:05 minikube kubelet[1445]: I1204 20:13:05.214135    1445 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Dec 04 20:13:16 minikube kubelet[1445]: E1204 20:13:16.036834    1445 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flask-app:latest"
Dec 04 20:13:16 minikube kubelet[1445]: E1204 20:13:16.037002    1445 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flask-app:latest"
Dec 04 20:13:16 minikube kubelet[1445]: E1204 20:13:16.037320    1445 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:flask-app,Image:flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:REDIS_HOST,Value:redis,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brk6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-deployment-f7b77cf85-j98q5_default(6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3): ErrImagePull: Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Dec 04 20:13:16 minikube kubelet[1445]: E1204 20:13:16.038924    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"
Dec 04 20:13:27 minikube kubelet[1445]: E1204 20:13:27.248533    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"flask-app:latest\\\"\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"
Dec 04 20:13:31 minikube kubelet[1445]: I1204 20:13:31.001024    1445 scope.go:117] "RemoveContainer" containerID="03cfb163c0577cf0e04b0da3bf3e3afd2f5bfac03dc2f39b5a9007df1ae018bd"
Dec 04 20:13:31 minikube kubelet[1445]: I1204 20:13:31.001366    1445 scope.go:117] "RemoveContainer" containerID="ef0231599d5a9172261dce2ef02e0122e413516d7c39bbadd7b058471121a4b6"
Dec 04 20:13:31 minikube kubelet[1445]: E1204 20:13:31.001508    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(2db3a54d-c960-4117-9923-da85335324b9)\"" pod="kube-system/storage-provisioner" podUID="2db3a54d-c960-4117-9923-da85335324b9"
Dec 04 20:13:41 minikube kubelet[1445]: E1204 20:13:41.626299    1445 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flask-app:latest"
Dec 04 20:13:41 minikube kubelet[1445]: E1204 20:13:41.626521    1445 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flask-app:latest"
Dec 04 20:13:41 minikube kubelet[1445]: E1204 20:13:41.626846    1445 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:flask-app,Image:flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:REDIS_HOST,Value:redis,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brk6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-deployment-f7b77cf85-j98q5_default(6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3): ErrImagePull: Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Dec 04 20:13:41 minikube kubelet[1445]: E1204 20:13:41.628121    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"
Dec 04 20:13:45 minikube kubelet[1445]: I1204 20:13:45.232827    1445 scope.go:117] "RemoveContainer" containerID="ef0231599d5a9172261dce2ef02e0122e413516d7c39bbadd7b058471121a4b6"
Dec 04 20:13:56 minikube kubelet[1445]: E1204 20:13:56.239278    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"flask-app:latest\\\"\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"
Dec 04 20:14:09 minikube kubelet[1445]: E1204 20:14:09.243198    1445 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"flask-app:latest\\\"\"" pod="default/flask-deployment-f7b77cf85-j98q5" podUID="6a2a98a4-c9ff-47d4-9372-d8e0b4588bf3"


==> storage-provisioner [49e6089b43a0] <==
I1204 20:13:45.344502       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1204 20:13:45.349794       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1204 20:13:45.349902       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1204 20:14:02.782699       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1204 20:14:02.783025       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_ecb8a27b-4b4b-44af-8eab-15380557d3d7!
I1204 20:14:02.783942       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ed1ad497-80bb-45eb-935c-eded0a18eac4", APIVersion:"v1", ResourceVersion:"3075", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_ecb8a27b-4b4b-44af-8eab-15380557d3d7 became leader
I1204 20:14:02.883618       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_ecb8a27b-4b4b-44af-8eab-15380557d3d7!


==> storage-provisioner [ef0231599d5a] <==
I1204 20:13:00.873246       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1204 20:13:30.879740       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

